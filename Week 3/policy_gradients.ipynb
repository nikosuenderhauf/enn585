{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENN585 - Advanced Machine Learning - Week 3\n",
    "\n",
    "Welcome to Week 3 of ENN585!\n",
    "\n",
    "This week's notebook let's you explore simple Policy Gradient concepts, including a simple implementation of REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we start by importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "\n",
    "# This is a helper function that let's us plot rewards and action distributions later on.\n",
    "def draw_plots(actions, rewards, total_rewards):\n",
    "\n",
    "    # plot the action probabilities\n",
    "    actions = np.array(actions); mu = actions.mean(axis=0); std = actions.std(axis=0)\n",
    "    action_labels = ['L', 'C', 'R']\n",
    "    for i in range(3):\n",
    "        plt.plot(mu[:,i], label=f'Action {action_labels[i]}')    \n",
    "        plt.fill_between(range(mu.shape[0]), mu[:,i]-std[:,i], mu[:,i]+std[:,i], alpha=0.2)        \n",
    "    plt.ylim(0,1.05)\n",
    "    plt.xlabel('Time Step'); plt.ylabel('Action Probabilities'); plt.legend(); plt.grid(); plt.tight_layout()\n",
    "\n",
    "    # plot rewards  \n",
    "    plt.figure()\n",
    "    rewards = np.array(rewards); mu = rewards.mean(axis=0); std = rewards.std(axis=0)\n",
    "    plt.plot(mu, color='black')\n",
    "    plt.fill_between(range(mu.shape[0]), mu-std, mu+std, alpha=0.2, color='black')\n",
    "    plt.grid(); plt.xlabel('Time Step'); plt.ylabel('Reward'); plt.tight_layout()\n",
    "\n",
    "    # plot total rewards\n",
    "    plt.figure()\n",
    "    plt.plot(total_rewards)\n",
    "\n",
    "\n",
    "# a helper function to plot the animations we used in the lecture slides\n",
    "def plot_animation(i):    \n",
    "    plt.clf()\n",
    "    action = trajectory[i][0]\n",
    "    plt.bar([0, 1, 2], action.flatten())\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xticks([0, 1, 2], ['L', 'C', 'R'])\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Action Distribution at Time {i}')\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ani = animation.FuncAnimation(fig, plot_animation, frames=list(range(0, len(trajectory[0:150]))), blit=False)\n",
    "# filename='ani-06.gif'\n",
    "# ani.save(filename, writer='imagemagick', fps=40)\n",
    "# plt.close()\n",
    "# display(Image(filename))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Policy Optimisation\n",
    "\n",
    "We used a very simple environment and policy in the lecture to illustrate the basic concepts of policy optimisation. The policy is essentially just a lookup table that outputs a probability distribution over the actions, but is independent of the state.\n",
    "\n",
    "In the Prac, explore the different concepts by recreating what we demonstrated during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A simple policy class. It is actually not depending on the state. Instead, it returns a fixed policy.\n",
    "# There are three actions, and the policy returns a probability distribution over these actions.\n",
    "# \n",
    "# Three parameters (the logits) are used to define the policy. \n",
    "# The softmax function is used to convert the logits to a probability distribution.\n",
    "class Policy:\n",
    "    def __init__(self):\n",
    "        self.logits = np.random.rand(3,1)\n",
    "        self.backward()\n",
    "\n",
    "    # the softmax function in numpy\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    # the gradient of the softmax function\n",
    "    def grad_softmax(self, x):\n",
    "        p = self.softmax(x)\n",
    "        return p * (1 - p)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # clamp the logits to 100 to avoid numerical overflow\n",
    "        self.logits = np.clip(self.logits, -100, 100)        \n",
    "        return self.softmax(self.logits)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = self.grad_softmax(self.logits)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        return self.forward(state)\n",
    "\n",
    "\n",
    "# The environment is super simple. There is no state, and the reward is only depending on the action.\n",
    "class EnvironmentSimple:\n",
    "    def __init__(self) -> None:\n",
    "        self.state = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # we only get a reward from action 0, and the state never changes\n",
    "        if action == 0:\n",
    "            reward = 1        \n",
    "        else:\n",
    "            reward = 0            \n",
    "        \n",
    "        # we never stop the episode\n",
    "        done = False\n",
    "\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TURN!**\n",
    "\n",
    "Below is the simple policy we used in the lecture. It's a simple lookup table that outputs a probability distribution over the actions, but is independent of the state. \n",
    "\n",
    "Change the update rule to the Policy Gradient update and recreate the steps we used in the lecture. Feel free to explore the different concepts by changing the environment and policy, or parameters like the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "rewards = []\n",
    "total_rewards = []\n",
    "\n",
    "lr = 0.2\n",
    "\n",
    "# we run the policy for 100 episodes\n",
    "for episode in range(100):    \n",
    "    trajectory = []    \n",
    "    env = EnvironmentSimple()\n",
    "    state = 0\n",
    "\n",
    "    # We re-initialise the policy for every episode, forgetting what we learned.\n",
    "    # Notice that this is NOT how RL algorithms would work in practice: you would want to keep learning after completing an episode and\n",
    "    # complete a large number of episodes, what we might call a \"run\". For the sake of this simple example, we reset the policy at the beginning of each episode.\n",
    "    policy = Policy()           \n",
    "    \n",
    "    # each episode has 500 time steps\n",
    "    for i in range(500):        \n",
    "\n",
    "        # run the policy to get the action probabilities\n",
    "        action = policy(state)        \n",
    "        \n",
    "        # sample from the action distribution\n",
    "        a = np.random.choice([0,1,2], p=action.flatten())\n",
    "        \n",
    "        # execute the policy in the environment\n",
    "        state, reward, done = env.step(a)        \n",
    "        trajectory.append([action, reward, a])        \n",
    "\n",
    "        # update the network weights                \n",
    "        # YOUR TURN! Change the update rule here to the policy gradient update. Retrace the steps we demonstrated in the lecture.\n",
    "        policy.logits[a] += 0 \n",
    "                \n",
    "\n",
    "    # some housekeeping for later plotting\n",
    "    actions.append(np.array([x[0] for x in trajectory]).squeeze())\n",
    "    rewards.append(np.array([x[1] for x in trajectory]).squeeze())\n",
    "    total_rewards.append(rewards[-1].sum())\n",
    "\n",
    "# plot the action probabilities\n",
    "draw_plots(actions, rewards, total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Update with a Neural Network\n",
    "\n",
    "Now we use an actual neural network to represent the policy. We will use the same environment as in the lecture, but now the policy is a neural network that takes the state as input and outputs a probability distribution over the actions. The network is still very simple, but it's a step up from the lookup table. \n",
    "\n",
    "Notice how we use PyTorch's abilities to calculate the gradients and perform parameter update steps for us. This is a very simple example, but it's a good illustration of how PyTorch can be used to implement more complex algorithms.\n",
    "\n",
    "**YOUR TURN!**\n",
    "\n",
    "Explore the different concepts by changing the network architecture, learning rate, or other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 3)\n",
    "        # YOUR TURN! Feel free to change the network architecture. Make sure you also update the forward() function accordingly.\n",
    "        # self.fc2 = nn.Linear(10, 3)       \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        return F.softmax(x, dim=0)\n",
    "        \n",
    "actions = []\n",
    "rewards = []\n",
    "total_rewards = []\n",
    "\n",
    "state = 0\n",
    "\n",
    "for episode in range(50):        \n",
    "    trajectory = []    \n",
    "    env = EnvironmentSimple()\n",
    "    policy = PolicyNetwork()    \n",
    "    optimiser = torch.optim.Adam(policy.parameters(), lr=1)    \n",
    "\n",
    "    for i in range(100):\n",
    "        optimiser.zero_grad()\n",
    "                \n",
    "        # get the action probabilities\n",
    "        action = policy(torch.tensor([state], dtype=torch.float32))        \n",
    "        \n",
    "        # sample from the action distribution                \n",
    "        a = torch.distributions.Categorical(action).sample().item()\n",
    "\n",
    "        # execute the action\n",
    "        state, reward, done = env.step(a)        \n",
    "        \n",
    "        # YOUR TURN! update the policy network with the loss and a gradient step                \n",
    "        loss = 0      \n",
    "        loss.backward()        \n",
    "        optimiser.step()\n",
    "\n",
    "        # remember the action probabilities, reward and the executed action\n",
    "        trajectory.append([action, reward, a])\n",
    "    \n",
    "    # some housekeeping to remember the results for later plotting\n",
    "    actions.append([x[0].detach().numpy() for x in trajectory])\n",
    "    rewards.append(np.array([x[1] for x in trajectory]).squeeze())\n",
    "    total_rewards.append(rewards[-1].sum())\n",
    "\n",
    "draw_plots(actions, rewards, total_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE: A Simple Policy Gradient Algorithm\n",
    "\n",
    "Now we are ready to explore REINFORCE, an actual policy gradient algorithm. \n",
    "\n",
    "We will introduce a more complicated environment, consisting of 3 states. The agent can transition back and forth between them. \n",
    "\n",
    "[ State A ] <--> [ State B ] <--> [ State C ]\n",
    "\n",
    "The agent can choose from the same actions as before, [Left, Center, Right]. This time, the agent always starts in the left-most state, has to move Right twice, and then invoke the Center action. It receives a positive reward then, but a slightly negative reward in every timestep.\n",
    "\n",
    "\n",
    "**YOUR TURN!**\n",
    "Experiment freely here to familiarise yourself to the concepts.\n",
    "\n",
    "- Experiment with different settings of gamma and learning rate.\n",
    "- Try to implement a baseline.\n",
    "- Try different network architectures.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentSequence:\n",
    "    def __init__(self) -> None:\n",
    "        # We represent the state as an integer, with -1 being the leftmost state, 0 the middle state, and 1 the rightmost state\n",
    "        self.state = -1\n",
    "\n",
    "    def step(self, action):        \n",
    "\n",
    "        # we have a sequence of states, and the agent needs to move to the right state        \n",
    "        if action == 0: # move left\n",
    "            self.state -= 1 \n",
    "        elif action == 1: # center, i.e. stay in the same state\n",
    "            self.state = self.state\n",
    "        elif action == 2: # move right\n",
    "            self.state += 1\n",
    "        \n",
    "        # make sure the state is always between -1 and 1\n",
    "        self.state = np.clip(self.state, -1, 1)\n",
    "\n",
    "        # we only get positive reward in the rightmost state when executing action 1 ('center'), then we end the episode\n",
    "        if self.state == 1 and action == 1:\n",
    "            reward = 1\n",
    "            done = True\n",
    "        else: # otherwise the agent gets a small negative reward for taking actions and moving around \n",
    "            reward = -0.05\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n",
    "    \n",
    "#  =========================================================\n",
    "# A very simple policy network. It takes the state as input and returns a probability distribution over the 3 actions.\n",
    "# YOUR TURN! Change the network architecture. Make sure you also update the forward() function accordingly.\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n",
    "#  =========================================================\n",
    "action_probs_to_plot = []\n",
    "rewards_to_plot = []\n",
    "total_rewards = []\n",
    "total_loss = []\n",
    "\n",
    "for runs in range(10):     \n",
    "\n",
    "    # change the random seed for every run, but it will be repeatable every time you run this notebook cell  \n",
    "    torch.manual_seed(runs)\n",
    "\n",
    "    #initialise the agent with every seed\n",
    "    policy = PolicyNetwork()    \n",
    "    optimiser = torch.optim.AdamW(policy.parameters(), lr=0.01)    \n",
    "\n",
    "    episode_reward = []\n",
    "    episode_loss = []\n",
    "    for episode in range(500):      \n",
    "        env = EnvironmentSequence() \n",
    "        state = env.state     \n",
    "\n",
    "        trajectory = {'state':[], 'action':[], 'reward':[], 'prob': []}        \n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # unroll the episode, i.e. execute the policy until the episode terminates \n",
    "        # notice how we remember the trajectory (states, actions, rewards, action probabilities) for later \n",
    "        done = False\n",
    "      \n",
    "        while not done:\n",
    "            # get the action probabilities from the policy\n",
    "            action = policy(torch.tensor([state], dtype=torch.float32))        \n",
    "\n",
    "            # sample from the action distribution                \n",
    "            a = torch.distributions.Categorical(action).sample().item()\n",
    "            prob = action[a]\n",
    "\n",
    "            # execute the action\n",
    "            state, reward, done = env.step(a)             \n",
    "            \n",
    "            # remember state, the executed action, and the reward\n",
    "            trajectory['state'].append(state)\n",
    "            trajectory['action'].append(a)\n",
    "            trajectory['reward'].append(reward)\n",
    "            trajectory['prob'].append(prob)\n",
    "           \n",
    "             \n",
    "        # After the episode ends, update the policy network with the policy gradient loss:\n",
    "        \n",
    "        # The Discount Factor. YOUR TURN! Experiment with the discount factor. What happens if you decrease it or increase it to 0.99?\n",
    "        gamma = 0.6 \n",
    "\n",
    "        # compute the future discounted return for each time step in the episode\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "        for R in trajectory['reward'][::-1]:\n",
    "            running_g = R + gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        # compute the policy gradient loss\n",
    "        deltas = torch.tensor(gs)\n",
    "        action_probs = trajectory['prob']   # remember all the action probabilities we stored in the trajectory\n",
    "        \n",
    "        # add up all the individual losses for each time step in the episode\n",
    "        loss = 0\n",
    "        for prob, delta in zip(action_probs, deltas):            \n",
    "            loss -= prob * delta    # YOUR TURN! Make changes to the loss function, e.g. explore the influence of a baseline term\n",
    "                \n",
    "        # perform a gradient update step on the policy network, using all of the data gathered in the episode\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        # # some housekeeping to remember the results for later plotting    \n",
    "        total_rewards.append(np.sum(trajectory['reward']))\n",
    "        episode_loss += [loss.detach()]\n",
    "\n",
    "\n",
    "    # after finishing an episode, print some information\n",
    "    total_loss += [episode_loss]\n",
    "    print(f'\\nRun {runs} - Total Reward over all Episodes: {np.sum(total_rewards[-episode:])} -- Final Loss: {np.mean(episode_loss)}')\n",
    "    print(f'Policy in state -1: {policy(torch.tensor([-1], dtype=torch.float32)).detach().tolist()}')       \n",
    "    print(f'Policy in state 0: {policy(torch.tensor([0], dtype=torch.float32)).detach().tolist()}')       \n",
    "    print(f'Policy in state 1: {policy(torch.tensor([1], dtype=torch.float32)).detach().tolist()}')          \n",
    "    print(f'Action sequence in the final episode: {trajectory[\"action\"]}')\n",
    "\n",
    "\n",
    "# after all the runs, plot how the loss evolved across all episodes, by plotting the mean loss and standard deviations across all runs\n",
    "mean_loss = np.mean(total_loss, axis = 0)\n",
    "std_loss = np.std(total_loss, axis = 0)\n",
    "plt.plot(mean_loss)    \n",
    "plt.fill_between(range(mean_loss.shape[0]), mean_loss-std_loss, mean_loss+std_loss, alpha=0.2)\n",
    "plt.xlabel('Episode'); plt.ylabel('Episode loss'); plt.grid(); plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Further\n",
    "\n",
    "Head to https://gymnasium.farama.org/tutorials/training_agents/reinforce_invpend_gym_v26/#sphx-glr-tutorials-training-agents-reinforce-invpend-gym-v26-py to explore a more complex environment (CartPole) for the REINFORCE algorithm.\n",
    "\n",
    "You can download a demo notebook with the code from the bottom of the website.\n",
    "\n",
    "\n",
    "## Connection to the Assessment 1 Project\n",
    "\n",
    "The concepts you learn in this week's prac are directly applicable to the first assessment project. You can implement a policy gradient algorithm to solve the Fetch Slide environment. Try adopting REINFORCE to this environment and see how it performs. Try initialising the policy network from your hand-crafted policy using the imitation learning techniques from Week 2 and see if it helps the learning process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enn585",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
