{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENN585 - Advanced Machine Learning - Week 4\n",
    "\n",
    "Welcome to Week 4 of ENN585!\n",
    "\n",
    "\n",
    "Implementing reinforcement learning algorithms yourself from scratch can be insightful and educational, especially for the basic algorithms. However, the implementation for more advanced algorithms can be quite complex and small implementation details that are often not described in the papers can have a large impact on the performance of the algorithm. Therefore, it is often more practical to use existing libraries.\n",
    "\n",
    "This week's notebook lets you explore one of those libraries, called Stable Baselined 3, or SB3 for short. It is a popular library for reinforcement learning and contains implementations of many state-of-the-art algorithms.\n",
    "\n",
    "You can access the documentation for SB3 [here](https://stable-baselines3.readthedocs.io/en/master/). This [blogpost](https://araffin.github.io/post/sb3/) is a good resource too, explaining the motivation behind the library and giving a short overview.\n",
    "\n",
    "In this notebook, we will use SB3 to train a reinforcement learning agent to solve the CartPole environment. The CartPole environment is a classic control problem, where the goal is to balance a pole on a cart. The environment is considered solved if the agent can balance the pole for 200 time steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Setup\n",
    "\n",
    "For this notebook you will need the [Stable Baselined 3](https://stable-baselines3.readthedocs.io/en/master/index.html) library, which you can install with `pip install stable-baselines3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install packages - (Run this once at the start)\n",
    "\n",
    "try:\n",
    "  import gymnasium as gym\n",
    "  gym.spec('FetchSlide-v2')\n",
    "except:\n",
    "  !pip install gymnasium-robotics\n",
    "  import gymnasium as gym\n",
    "\n",
    "try:\n",
    "    import table_baselines3\n",
    "except:\n",
    "    !pip install stable-baselines3     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps\n",
    "\n",
    "Let's start by training an Actor-Critic agent using the Advantage-Actor-Critic algorithm (A2C). \n",
    "\n",
    "The A2C algorithm is a popular reinforcement learning algorithm that is based on the policy gradient method. It is an on-policy algorithm, meaning that it learns from the data that it collects with the current policy. The algorithm uses a value function to estimate the expected return of a state, and an actor to select actions. The actor and the critic are trained together, and the algorithm uses the advantage function to estimate the quality of the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# createing an RL agent is easy, notice how the environment is passed to the agent\n",
    "model = A2C(\"MlpPolicy\", env)\n",
    "\n",
    "# Train the agent, this should take around a minute\n",
    "print('Training ...')\n",
    "model.learn(total_timesteps=20_000)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the agent, let's see it in action and test it out. \n",
    "\n",
    "Notice how we can call `model.predict()` to get the action that the agent wants to take in a given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re-use the environment from above, or just createa a new one\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "# we will perform 10 test runs\n",
    "for run in range(10):\n",
    "\n",
    "    # make sure to reset the environment before each run\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # we can use the model to predict the next action\n",
    "        action, _ = model.predict(observation)\n",
    "        \n",
    "        # and then apply the action to the environment\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "\n",
    "        # we stop the loop if we terminate or run out of time (truncated)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print(f'Agent {run} achieved a total reward of: {total_reward}')\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "\n",
    "print(f'Average reward over {run+1} runs: {np.mean(total_rewards)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful feature is to generate a .gif that shows the agent in action. \n",
    "\n",
    "Notice how we use the `env.render()` function to render the environment into an image, save all the images into a list, and then use the `imageio` library to save the list of images into a .gif file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "images = []\n",
    "\n",
    "while True:\n",
    "    # we can use the model to predict the next action\n",
    "    action, _ = model.predict(observation)\n",
    "    \n",
    "    # and then apply the action to the environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # render the environment and store the image in a list\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "\n",
    "    # we stop the loop if we terminate or run out of time (truncated)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "# now we can generate a .gif animation from all the images\n",
    "imageio.mimsave(\"cartpole_a2c.gif\", [np.array(img) for img in images], fps=29)\n",
    "\n",
    "# we can also visualise the gif in the notebook\n",
    "from IPython.display import Image\n",
    "display(Image('cartpole_a2c.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving Deeper into SB3\n",
    "\n",
    "### Changing the Network Architectures \n",
    "\n",
    "SB3 provides default architectures for its policy and value networks. However, it is possible to change the architecture of the networks in a simple way. \n",
    "\n",
    "This [documentation page](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html) provides details on how to create custom network architectures.\n",
    "Have a look how to do this, especially for the Actor-Critic agents that need two networks (see [here](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#on-policy-algorithms))\n",
    "\n",
    "\n",
    "### Speeding Up Training with Parallel Worker Threads\n",
    "\n",
    "Some algorithms (e.g. A2C but also PPO or SAC) can be parallelized to speed up training. This is done by using multiple worker threads that collect data in parallel. \n",
    "\n",
    "The code cell below demonstrates how to use parallel worker threads in SB3. Compare the timing of training with and without parallel worker threads.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "# train A2C using a single environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "model = A2C(\"MlpPolicy\", env)\n",
    "print('Training A2C with a single environment...')\n",
    "%time model.learn(total_timesteps=10_000)\n",
    "\n",
    "# train A2C using 8 parallel environments\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=8)\n",
    "model = A2C(\"MlpPolicy\", env)\n",
    "print('\\nTraining A2C with 8 parallel environments...')\n",
    "%time model.learn(total_timesteps=10_000)\n",
    "\n",
    "# train A2C using 8 parallel environments, but force it to be on the cpu\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = A2C(\"MlpPolicy\", env, device=\"cpu\")\n",
    "print('\\nTraining A2C with 8 parallel environments on the CPU ...')\n",
    "%time model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Performance of Different Algorithms\n",
    "\n",
    "**YOUR TURN!**\n",
    "\n",
    "Now that you have seen how to train an A2C agent, try training agents using different algorithms.\n",
    "\n",
    "When choosing algorithms, be aware that some are designed for discrete action spaces, while others are designed for continuous action spaces. The CartPole environment has a discrete action space, so you have to choose algorithms that are designed for discrete action spaces.\n",
    "\n",
    "Task:\n",
    "- Train 3 agents, using 3 different algorithms: A2C, PPO, and DQN.\n",
    "- Compare the performance of the agents by evaluating them over 10 runs each and reporting the average total reward for them.\n",
    "- Change the number of training steps and the number of parallel worker threads to see how it affects the training time and the performance of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN!\n",
    "\n",
    "# Modify the code cells above to train and evaluate agents using three different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Discrete Action Spaces\n",
    "\n",
    "The CartPole environment has a discrete action space, but many environments have continuous action spaces. This includes the FetchSlide environment you are using for your Assessment 1 project.\n",
    "\n",
    "**YOUR TURN**\n",
    "\n",
    "Explore the algortihms in SB3 that are designed for continuous action spaces. Train and compare agents using these algorithms on a continuous action space environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enn585",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
