{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaxgY4MMj4uN"
      },
      "source": [
        "# ENN585 - Advanced Machine Learning - Week 1\n",
        "\n",
        "In the first half of this semester, you will be working with the [Gymnasium](https://gymnasium.farama.org/#) package, an API standard for reinforcement learning with a diverse collection of reference environments. It is commonly referred to as `gym` for short.\n",
        "\n",
        "In addition to classical RL problems like the Cart Pole or a double inverted pendulum that are contained in the base Gymnasium package, [Gymnasium-Robotics](https://robotics.farama.org/) contains environments specifically for robot experiments.\n",
        "\n",
        "This notebook helps you to familiarise yourself with Gymnasium and specifically its robotics environments. We strongly encourage you to explore further and run your own experiments with the help of their documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Setup\n",
        "\n",
        "You can run this notebook on [Google Colab](https://colab.google/) or locally on your computer.\n",
        "\n",
        "This first code cell takes care of some installation and setup. If you run this locally on your own machine, we recommend setting up a conda environment and installing ```pip install gymnasium-robotics renderlab```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0jZ247OZJj9",
        "outputId": "b1e58f39-79cf-44dc-9174-37dbfad1b230"
      },
      "outputs": [],
      "source": [
        "#@title Install packages - (Run this once at the start)\n",
        "\n",
        "### Install gym-robotics and renderlab\n",
        "try:\n",
        "  import gymnasium as gym\n",
        "  gym.spec('FetchSlide-v2')\n",
        "except:\n",
        "  !pip install gymnasium-robotics\n",
        "  import gymnasium as gym\n",
        "\n",
        "try:\n",
        "  import renderlab as rl\n",
        "except:\n",
        "  !pip install renderlab\n",
        "\n",
        "\n",
        "## Are we on Google Colab?\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "\n",
        "### If on Colab, we have to setup gym's rendering. Otherwise we are ok to proceed.\n",
        "if IN_COLAB:\n",
        "\n",
        "  from google.colab import files\n",
        "  import distutils.util\n",
        "  import os\n",
        "  import subprocess\n",
        "  if subprocess.run('nvidia-smi').returncode:\n",
        "    raise RuntimeError(\n",
        "        'Cannot communicate with GPU. '\n",
        "        'Make sure you are using a GPU Colab runtime. '\n",
        "        'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "  # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "  # This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "  # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "  # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "  NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "  if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "      f.write(\"\"\"{\n",
        "      \"file_format_version\" : \"1.0.0\",\n",
        "      \"ICD\" : {\n",
        "          \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "      }\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "  # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "  print('Setting environment variable to use GPU rendering:')\n",
        "  %env MUJOCO_GL=egl\n",
        "\n",
        "  try:\n",
        "    print('Checking that the installation succeeded:')\n",
        "    import mujoco\n",
        "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "  except Exception as e:\n",
        "    raise e from RuntimeError(\n",
        "        'Something went wrong during installation. Check the shell output above '\n",
        "        'for more information.\\n'\n",
        "        'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "        'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "  print('Installation successful.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A8PQUEqjmMC"
      },
      "source": [
        "# Let's Explore Some Environments\n",
        "Gymnasium offers a number of interesting environments we can use for imitation learning and reinforcement learning experiments.\n",
        "Let's have a look at some of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZNdM4OPdZRxW",
        "outputId": "75df4865-f91f-4698-bc87-1e42feb41a8e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import renderlab as rl\n",
        "\n",
        "# we'll try these environments\n",
        "# if we want to see more, try running gym.\n",
        "env_names = ['CartPole-v1', 'FetchSlide-v2', 'FrankaKitchen-v1', 'Humanoid-v4']\n",
        "\n",
        "for name in env_names:\n",
        "  print(f'============ Environment: {name} ============')\n",
        "\n",
        "  # create an environment\n",
        "  env = gym.make(name, render_mode='rgb_array')\n",
        "\n",
        "  # this wraps the environment so we can record a video of its outputs and watch it later\n",
        "  env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "  # reset the environment, this needs to be called at least once in the beginning\n",
        "  observation, info = env.reset()\n",
        "\n",
        "  # do a loop\n",
        "  while True:\n",
        "    # sample a random action to be executed\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # this executes the action and returns observation and reward etc\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # we stop the loop if we terminate (e.g. the pole falls over) or run out of time (truncated after 50 steps)\n",
        "    if terminated or truncated:\n",
        "      break\n",
        "\n",
        "  # show the recorded video\n",
        "  env.play()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore other Environments\n",
        "\n",
        "Gym comes with support for many other environments. Have a look at the documentation https://gymnasium.farama.org/ \n",
        "The code cell below prints all the available environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqguDGoIZUD1",
        "outputId": "6f05b843-fe3e-4d52-cc17-d16fd423d559"
      },
      "outputs": [],
      "source": [
        "# print all the available environments in gym\n",
        "gym.pprint_registry()\n",
        "\n",
        "# feel free to create and explore other environments, using the code block from above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the Action Space of the Fetch Environment\n",
        "\n",
        "Let's examine the Fetch-Slide environment. This is the environment you will be using for your Assessment 1 project.\n",
        "\n",
        "We will start with looking at the action space. Let's make the robot move!\n",
        "\n",
        "\n",
        "### Your Turn!\n",
        " - Change the code so that the robot moves upwards, downwards, left, right, forward, backwards.\n",
        " - Try different speeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5JBAEjDZY0R"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FetchSlide-v2', render_mode='rgb_array')\n",
        "\n",
        "# this wraps the environment so we can record a video of its outputs and watch it later\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "# reset the environment, this needs to be called at least once in the beginning\n",
        "observation, info = env.reset()\n",
        "\n",
        "# let's see what the actions looks like\n",
        "# see https://robotics.farama.org/envs/fetch/slide/#action-space for more details\n",
        "random_action = env.action_space.sample()\n",
        "print(f'sampeled action:{action}')\n",
        "\n",
        "\n",
        "# do a loop\n",
        "while True:\n",
        "    # always excecute a specific action \n",
        "    # change this to make the robot do something else, e.g. move up or left or forward/backward. Use different speeds.\n",
        "    action = [0,0,0,0]    # [dx, dy, dz, gripper]\n",
        "\n",
        "    # this executes the action and returns observation and reward etc\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # we stop the loop if we terminate or run out of time (truncated)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
        "env.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the Observation Space of the Fetch Environment\n",
        "Read the documentation for more details: https://robotics.farama.org/envs/fetch/slide/#observation-space\n",
        "\n",
        "### Your Turn! \n",
        " - Use the observations to implement a simple hand-written controller that pushes the block towards the direction of the goal.\n",
        " - Save and plot the position of the robot's end effector, the puck and the desired goal throughout the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# a little helpe function to implement a simple P controller\n",
        "# feel free to make this more sophisticated, e.g. a PI or PID controller\n",
        "def p_controller(pos, goal, kp=1.0):\n",
        "    error = goal - pos\n",
        "    dist = np.linalg.norm(error)\n",
        "    action = error * kp * dist\n",
        "    return action, dist < 0.01\n",
        "\n",
        "# create the environment\n",
        "env = gym.make('FetchSlide-v2', render_mode='rgb_array',  max_episode_steps=200)    # notice how we set the max_episode_steps to 200 to have more time in the simulation\n",
        "\n",
        "# this wraps the environment so we can record a video of its outputs and watch it later\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "# reset the environment, this needs to be called at least once in the beginning\n",
        "observation, info = env.reset()\n",
        "\n",
        "\n",
        "# Here is a simple idea of how to push the puck to a goal position:\n",
        "# 1. lift the robot's end effector up\n",
        "# 2. move the robot's end effector to a pre-defined start position\n",
        "# 3. move the robot's end effector behind the puck, so that it is aligned with the desired goal\n",
        "# 4. push the puck towards the goal position\n",
        "\n",
        "# We will implement this as a simple state machine, where we have different phases\n",
        "phases=[' lift', 'start', 'move_behind', 'push']\n",
        "phase = phases[0]\n",
        "\n",
        "# do a loop\n",
        "while True:\n",
        "    # we will implement a simple state machine to control the robot\n",
        "        \n",
        "    if phase == 'lift':\n",
        "        # the current position of the gripper\n",
        "        ef = observation['observation'][:3]\n",
        "        # the goal position of the gripper is where it currently is, but with a higher z value\n",
        "        goal = ef.copy(); goal[2] = 0.6\n",
        "        # use the p controller to move the gripper to the goal position\n",
        "        action, converged = p_controller(ef, goal, kp=100.0)        \n",
        "        # if we are close enough to the goal position, we move to the next phase\n",
        "        if converged:\n",
        "            phase = phases[1]\n",
        "    \n",
        "    # the next phases are similar, we just have different goals\n",
        "    elif phase == 'start':\n",
        "        ef = observation['observation'][:3]\n",
        "        goal = [0.8, 0.75,0.42] # this is a pre-defined start position\n",
        "        \n",
        "        # Again, use the p controller to move the gripper to the goal position. Try different kp values.\n",
        "        action, converged = p_controller(ef, goal, kp=100.0)        \n",
        "        \n",
        "        # if we are close enough to the goal position, we move to the next phase\n",
        "        if converged:\n",
        "            phase = phases[2]\n",
        "    \n",
        "    # the next phases are similar, we just have different goals\n",
        "    elif phase == 'move_behind':        \n",
        "        # TODO: implement this phase\n",
        "        action = [0,0,0]\n",
        "\n",
        "    elif phase == 'push':     \n",
        "        # TODO: implement this phase\n",
        "        action = [0,0,0]\n",
        "    else:\n",
        "        action = [0,0,0]\n",
        "    \n",
        "    # add one more element to the action vector (gripper open/close)\n",
        "    action = np.concatenate([action, [0]]) \n",
        "    \n",
        "    # this executes the action and returns observation and reward etc\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # we stop the loop if we terminate or run out of time (truncated)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "    \n",
        "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
        "env.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the Rewards of the Fetch Environment\n",
        "Read the documentation for more details: https://robotics.farama.org/envs/fetch/slide/#rewards\n",
        "\n",
        "Let's have a look at the reward signal we get from the environment.\n",
        "\n",
        "The environment we have used so far produces *sparse* rewards. It is 0 if the puck is within 5cm of the desired goal position, and -1 otherwise.\n",
        "\n",
        "We can also configure the environment to return *dense* rewards. In this case, the reward is the negative Euclidean distance between the achieved puck position and the desired goal.\n",
        "To create the dense reward version of the environment, we have to use `env = gym.make('FetchSlideDense-v2')` when creating it, instead of the `env = gym.make('FetchSlide-v2')` we used so far.\n",
        "\n",
        "### Your Turn! \n",
        "Modify the code block below.\n",
        " - Run 100 experiments with random actions. Log all the rewards (after each action). Then generate some insightful statistics, and include some plots of the reward signal. Do random actions generate any positive reward signal? What do you think that means if we try to learn based on the reward?\n",
        " - Next, rerun the experiments with the dense-reward version of the environment. Compare the results. Would learning from dense rewards be easier?\n",
        " - Use your hand-written simple controller from above. What rewards does it achieve? \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('FetchSlide-v2', render_mode='none')  # render_mode none means we do not generate any videos, which speeds up the simulation\n",
        "\n",
        "n_experiments = 10\n",
        "\n",
        "for i in range(n_experiments):\n",
        "\n",
        "    print(f'Running experiment {i+1} of {n_experiments} ...')\n",
        "\n",
        "    # reset the environment\n",
        "    observation, info = env.reset()\n",
        "\n",
        "    # do a loop\n",
        "    while True:\n",
        "        # execute a random action\n",
        "        random_action = env.action_space.sample()\n",
        "\n",
        "        # this executes the action and returns observation and reward etc\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # we stop the loop if we terminate or run out of time (truncated)\n",
        "        if terminated or truncated:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
