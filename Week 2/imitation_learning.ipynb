{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENN585 - Advanced Machine Learning - Week 2\n",
    "\n",
    "Welcome to Week 2 of ENN585!\n",
    "\n",
    "In this notebook we will explore Imitation Learning -- the topic of this week.\n",
    "\n",
    "You will train an imitation learning agent for a simple Cart Pole balancing task, and a more complex task involving a opening a door with a robotic hand.\n",
    "\n",
    "In the end, you can build on your code from Week 1 and train a policy network based on the demonstrations by your hand-written controller in the Fetch-Slide environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Setup\n",
    "\n",
    "You can run this notebook on [Google Colab](https://colab.research.google.com/github/nikosuenderhauf/enn585/blob/main/Week%202/imitation_learning.ipynb) or locally on your computer.\n",
    "\n",
    "This first code cell takes care of some installation and setup. If you run this locally on your own machine, we recommend setting up a conda environment and following the setup instructions on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install packages - (Run this once at the start)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "  import minari\n",
    "except:\n",
    "  !pip install minari\n",
    "  import minari\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "### Install gym-robotics and renderlab\n",
    "try:\n",
    "  import gymnasium as gym\n",
    "  gym.spec('FetchSlide-v2')\n",
    "except:\n",
    "  !pip install gymnasium-robotics\n",
    "  import gymnasium as gym\n",
    "\n",
    "try:\n",
    "  import renderlab as rl\n",
    "except:\n",
    "  !pip install renderlab\n",
    "  import renderlab as rl\n",
    "\n",
    "## Are we on Google Colab?\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "\n",
    "### If on Colab, we have to setup gym's rendering. Otherwise we are ok to proceed.\n",
    "if IN_COLAB:\n",
    "  \n",
    "  # download the dataset with the expert demonstrations\n",
    "  !wget -P CartPole-v1-expert https://github.com/nikosuenderhauf/enn585/raw/main/Week%202/CartPole-v1-expert/data/main_data.hdf5 CartPole-v1-expert\n",
    "\n",
    "\n",
    "  from google.colab import files\n",
    "  import distutils.util\n",
    "  import os\n",
    "  import subprocess\n",
    "  try:\n",
    "    if subprocess.run('nvidia-smi').returncode:\n",
    "      raise RuntimeError(\n",
    "          'Cannot communicate with GPU. '\n",
    "          'Make sure you are using a GPU Colab runtime. '\n",
    "          'Go to the Runtime menu and select Choose runtime type.')\n",
    "  except:\n",
    "    pass\n",
    "  # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "  # This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "  # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "  # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "  NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "  if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "      f.write(\"\"\"{\n",
    "      \"file_format_version\" : \"1.0.0\",\n",
    "      \"ICD\" : {\n",
    "          \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "      }\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "  # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "  print('Setting environment variable to use GPU rendering:')\n",
    "  %env MUJOCO_GL=egl\n",
    "\n",
    "  try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    import mujoco\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "  except Exception as e:\n",
    "    raise e from RuntimeError(\n",
    "        'Something went wrong during installation. Check the shell output above '\n",
    "        'for more information.\\n'\n",
    "        'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "        'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "\n",
    "  print('Installation successful.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore the CartPole environment.\n",
    "\n",
    "Our first imitation learning example will use the simple Cart Pole environment, a classical task in control and learning.\n",
    "\n",
    "The task here is to balance an inverted pendulum by moving the cart to either the left or the right. The agent received a reward of +1 for every time step the pendulum is not tipped beyond a certain angle. Otherwise the episode ends. \n",
    "\n",
    "The maximum reward the agent can get is 500, since the episode is terminated after 500 timesteps.\n",
    "\n",
    "Let's have a look by executing some random actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# this wraps the environment so we can record a video of its outputs and watch it later\n",
    "env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "# reset the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "# we will keep track of the accumulated reward \n",
    "accumulated_reward = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    # sample a random action to be executed\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # this executes the action and returns observation and reward etc\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # increment the accumulated reward\n",
    "    accumulated_reward += reward\n",
    "\n",
    "    # we stop the loop if we terminate (e.g. the pole falls over) or run out of time (truncated after 50 steps)\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "\n",
    "# show the recorded video    \n",
    "env.play()\n",
    "\n",
    "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
    "print(f'Total reward received: {accumulated_reward}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards an Imitation Learning Agent\n",
    "\n",
    "Instead of executing randomly sampled actions, let's introduce a policy network that acts as the agent.\n",
    "\n",
    "The policy network $\\pi(a|s)$ returns actions, given the state of the environment. \n",
    "We will define a simple neural network for the policy network, but for more complex tasks the network architecture will of course be much more involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a Policy Network \\pi(a|s) that takes in a state and outputs a probability distribution over actions.\n",
    "# We can change the architecture of the network as we like, but for this example, we will use a simple 3-layer fully connected network.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this network to decide on the actions. In the beginning, the network will not be trained at all, so we can expect it to perform just as bad as random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# this wraps the environment so we can record a video of its outputs and watch it later\n",
    "env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "# here we create an instance of the policy network\n",
    "# notice how we define the input and output dimensions depending on the size of the observation and action space\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), action_space.n)\n",
    "\n",
    "# reset the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "accumulated_reward = 0\n",
    "while True:\n",
    "\n",
    "    # get the action from the policy network\n",
    "    # Question: why do we call .argmax() here? Inspect the output of the policy network and see if you can figure it out.\n",
    "    action = policy_net(torch.tensor(observation)).argmax()\n",
    "\n",
    "    # this executes the action and returns observation and reward etc\n",
    "    observation, reward, terminated, truncated, info = env.step(action.numpy())\n",
    "    accumulated_reward += reward\n",
    "\n",
    "    # we stop the loop if we terminate (e.g. the pole falls over) or run out of time (truncated after 50 steps)\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "\n",
    "# show the recorded video    \n",
    "env.play()\n",
    "\n",
    "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
    "print(f'Total reward received: {accumulated_reward}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's inspect the training dataset.\n",
    "\n",
    "We are using [Minari](https://minari.farama.org/), a Python API that hosts a number of popular datasets for offline reinforcement learning and imitation learning.\n",
    "\n",
    "We load a dataset for Cart Pole that was collected by an expert and replay one of the episodes.\n",
    "\n",
    "See how we can work with tha dataset in the code cell below, but also check out the documentation on Minari's website, e.g. https://minari.farama.org/content/basic_usage/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset containing the expert demonstrations\n",
    "%env MINARI_DATASETS_PATH=.\n",
    "dataset = minari.load_dataset('CartPole-v1-expert/')\n",
    "\n",
    "# recreate the environment used to generate the dataset\n",
    "env = dataset.recover_environment(render_mode='rgb_array')\n",
    "env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "# get one random episode from the dataset\n",
    "episode = dataset.sample_episodes(n_episodes=1)[0]\n",
    "\n",
    "# now use the episode data to visualize the expert's behavior\n",
    "# we reset the environment using the random seed of the episode, so we get the same initial state\n",
    "observation, info = env.reset(seed = episode.seed)\n",
    "\n",
    "\n",
    "accumulated_reward = 0\n",
    "\n",
    "# for all the actions in the episode ...\n",
    "for action in episode.actions:\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    accumulated_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.play()\n",
    "\n",
    "# this time we get much higher rewards, as the expert has learned to solve the task\n",
    "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
    "print(f'Total reward received: {accumulated_reward}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Behavioral Cloning\n",
    "\n",
    "Our next step is to set up a training loop and train the policy network, using the expert demonstrations. \n",
    "\n",
    "The ``MinariDataset`` is compatible with the PyTorch Dataset API, allowing us to load it directly using [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "\n",
    "However, since each episode can have a varying length, we need to pad them.\n",
    "To achieve this, we can utilize the [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) feature of PyTorch DataLoader. Let's create the ``collate_fn`` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {        \n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        )      \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to instantiate the data loader, create the training loop and train the network.\n",
    "\n",
    "You can experiment here by changing the number of epochs or the network architecture above. How does that influence performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a DataLoader that will iterate over the dataset in batches\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# the optimizer will be used to update the policy network\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "\n",
    "# we use a cross-entropy loss like in a classification task, as the action space is discrete\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# we train the policy network for 32 epochs, i.e. we iterate over the dataset 32 times\n",
    "num_epochs = 32\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        a_pred = policy_net(batch['observations'][:, :-1])\n",
    "        a_hat = F.one_hot(batch[\"actions\"]).type(torch.float32)\n",
    "        loss = loss_fn(a_pred, a_hat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the trained policy network in action. We will use the same environment as before, but this time we will use the policy network to select the actions.\n",
    "\n",
    "**YOUR TURN**\n",
    "\n",
    "Instead of just running a single episode, can you modify the code so that it runs 100 episodes and logs the received reward for each episode? Then you can report the average reward and standard deviation. This would be beneficial if you want to experiment with network architectures or details of the training loop, as you can very easily judge the change in performance.\n",
    "\n",
    "Hint: turn off the rendering by setting render_mode to `None` and skip the wrapping with the `RenderFrame` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# this wraps the environment so we can record a video of its outputs and watch it later\n",
    "env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "accumulated_reward = 0\n",
    "\n",
    "# for all the actions in the episode ...\n",
    "while True:\n",
    "\n",
    "    action = policy_net(torch.tensor(observation)).argmax()\n",
    "    observation, reward, terminated, truncated, info = env.step(action.numpy())\n",
    "    accumulated_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.play()\n",
    "\n",
    "# this time we get much higher rewards, as the network has learned to solve the task from the expert demonstrations\n",
    "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
    "print(f'Total reward received: {accumulated_reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a Different Environment -- Opening a Door with a Robot Hand\n",
    "\n",
    "Now that we have demonstrated behavioral cloning for the simple Cart Pole environment, we can look at a more complex task: We will learn how to open a door with a robot hand.\n",
    "\n",
    "The environment we will work with is https://robotics.farama.org/envs/adroit_hand/adroit_door/\n",
    "\n",
    "Have a look at the documentation, especially the parts about the action and state spaces.\n",
    "\n",
    "\n",
    "**YOUR TURN**\n",
    "Follow the instructions to edit the code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the dataset containing the expert demonstrations\n",
    "dataset = minari.load_dataset('door-expert-v1', download=True)\n",
    "\n",
    "# we recreate the environment used to generate the dataset\n",
    "env = dataset.recover_environment(render_mode='rgb_array')\n",
    "\n",
    "# as before, we generate a policy network and make sure it has the correct input and output dimensions\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), np.prod(action_space.shape))\n",
    "\n",
    "\n",
    "## YOUR TURN!\n",
    "# Using the code blocks above as a reference, start the environment and use the untrained policy network to control the agent.\n",
    "# Visualise the result using the video player as before.\n",
    "\n",
    "\n",
    "## YOUR TURN!\n",
    "# Now replay some of the recorded episodes from the dataset to see how the expert behaves in the environment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the policy network from the expert demonstrations. \n",
    "\n",
    "We can re-use the `collate_fn` function from above to create the `DataLoader` but have to choose a different loss function. \n",
    "\n",
    "**YOUR TURN**\n",
    "\n",
    "Choose an appropriate loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataLoader that will iterate over the dataset in batches\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# check if we have cuda available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# a new policy net instance\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), np.prod(action_space.shape)).to(device)\n",
    "\n",
    "\n",
    "# the optimizer will be used to update the policy network\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "\n",
    "\n",
    "# YOUR TURN! Choose a loss function that is appropriate for the task and implement the training loop.\n",
    "loss_fn = None\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# we train the policy network for 16 epochs, i.e. we iterate over the dataset 16 times\n",
    "# YOUR TURN! You can experiment and change the number of epochs, batch size, etc. and see how that influences the performance of the trained network.\n",
    "num_epochs = 16\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # the predicted action according to the policy network\n",
    "        a_pred = policy_net(batch['observations'][:, :-1,:].float().to(device))\n",
    "        \n",
    "        # the true action by the expert\n",
    "        a_hat = batch[\"actions\"].float().to(device)\n",
    "        \n",
    "        # The loss should measure the difference between both. Make sure you choose an appropriate loss function.\n",
    "        loss = loss_fn(a_pred, a_hat)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # update the policy network using the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the policy network we just trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('AdroitHandDoor-v1', render_mode='rgb_array')\n",
    "\n",
    "# this wraps the environment so we can record a video of its outputs and watch it later\n",
    "env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "# as always, we reset the environment and start the episode\n",
    "observation, info = env.reset()\n",
    "accumulated_reward = 0\n",
    "\n",
    "while True:\n",
    "    action = policy_net(torch.tensor(observation).float().to(device))\n",
    "  \n",
    "    observation, reward, terminated, truncated, info = env.step(action.detach().cpu().numpy())\n",
    "    accumulated_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.play()\n",
    "\n",
    "print(f'Episode ended because it was terminated: {terminated} or truncated: {truncated}')\n",
    "print(f'Total reward received: {accumulated_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect your own Dataset for Imitation Learning and Reinforcement Learning\n",
    "\n",
    "Last week you wrote a rudimentary controller that could do somewhat better than random actions at the Fetch-Slide task.\n",
    "\n",
    "Let's use Minari's `DataCollector` class to collect a dataset from this controller.\n",
    "Then, adapt the code blocks above to train a poicy network that can imitate your hand-written controller. This policy network can serve as a starting point for your reinforcement learning experiments in Assessment 1.\n",
    "\n",
    "First, check the documentation at https://minari.farama.org/tutorials/using_datasets/behavioral_cloning/#dataset-generation to see how a dataset can be collected. See how the `DataCollector` class wraps the environment while you execute actions from your 'expert' (hand-written) controller? The dataset gets stored automatically to `~/.minari/datasets` from where you can load it later using `minari.load_dataset()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN!\n",
    "# Follow the instructions above to:\n",
    "# - copy the code from last week that runs your hand-written controller on the FetchSlide environment\n",
    "# - use the DataCollector class to collect a dataset of expert demonstrations from this contoller\n",
    "# - train a policy network using the expert demonstrations\n",
    "# - visualize the trained policy network controlling the FetchSlide environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enn585",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
